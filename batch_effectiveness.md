# Exploring Batching Effectiveness
This is the documentation on using and interpreting `batch_effectiveness.py`

## Overview `batch_effectiveness.py`
`batch_effectiveness.py` allow a user to provide soundness model, folding factor, and number of variables of a list of polynomials, and compute the prover and verifier cost across different batching models (see below).

To run the script, modify the inputs within the file and run `$ python batch_effectiveness.py`.
> Let me know if you want to support command line input or more advanced input formats.

## Cost Model
Below, unless specified otherwise, "size of polynomial" refers to the number of coefficients (exponential to number of variables), and "cost" refers to number of field elements computed or stored.

We divide the cost of a batched WHIR into three categories.

### Merkle Tree
This is a cost specific to prover, computed as the size (number of bytes) of all Merkle Trees generated by the prover for each polynomial and / or for each round:
- every leaf on starting polynomials is 8 bytes * `folding_factor` * `num_polys`
- every leaf on folded polynomials is 16 bytes
- every intermediate node is 32 bytes

### Sumcheck
The cost of sumcheck is separated between `eval size` (for prover) and `round` for verifier. For each sumcheck, `eval size` is the sum of the size of the polynomials generated each round (i.e. 2 * size of the starting polynomial - 1). `total sumcheck eval size` is the aggregation of all such values across all sumchecks. `total sumcheck round` is the sum of number of rounds of all sumchecks, simulating the communication and verifier cost.

### Query
This is the sum of `num_queries * merkle_depth` across every WHIR round, applies to both the prover and the verifier.

## Batching Strategies
For every test case, we first present our baseline:
- `no_batch` is the cost of opening each polynomial separately

Then we present our batching scenarios:
- `batch no pad` does not perform any pad and "unifies in" polynomials at their respsective domain size. If the big polynomials finish before reaching the domain size of the small polynomials, a new WHIR proof is initiated starting at the largest small polynomial. This shows the minimum merkle tree size one can obtain.
- `batch all pad` pads all polynomials to the largest domain and performs one unification sumcheck. This shows the minimum number of queries one can obtain.
- `batch threshold <t>` allows the prover to perform padding up to `t / 100` of starting domain size. The prover always pads the smaller polynomials first and only pads if _all polynomials of the smallest domain can be padded to the second-smallest domain without exceeding the threshold_ (padding only some polynomials of a specific domain does not help at all). After all padding, the protocol continues like `batch no pad`.  
We note that `t` is an overestimate on prover cost blowup, since it does not account for the merkle trees generated on intermediate WHIR rounds, which the prover needs to pay regardless of padding. `total merkle tree size` comparison between `no pad` and `threshold` provides a more accurate picture.

## Main Takeaway
The tradeoff between prover cost on merkle tree and verifier cost on queries is quite linear: larger starting domains always lead to proportionally fewer queries. There does not appear to be a "sweet spot" or diminishing return. In general, even a 25% increase in merkle tree size can lead to significantly reduction on the number of queries. We can incorporate this "threshold" feature into WHIR to allow programmer fine-tuning.